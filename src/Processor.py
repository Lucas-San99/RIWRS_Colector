# ==============================================================================
# Corpo L칩gico do Coletor
# ==============================================================================
import os
import shutil
import hashlib
import datetime
import warnings
import pandas as pd
from tqdm import tqdm
from concurrent.futures import ThreadPoolExecutor, as_completed

# Importa칞칫es dos m칩dulos internos (assumindo que est칚o em src/)
from Verificador import Verificador
from Relatorio import GeradorRelatorio
from Logging import setup_logging

# Configurac칚o do logger (Chamada modular, garantindo que seja o mesmo do launcher)
logger = setup_logging()

# ==============================================================================
# 2. CONFIGURA칂칏ES PRINCIPAIS
# ==============================================================================

# --- Caminhos e Arquivos ---
BASE_PATH = os.path.dirname(os.path.dirname(os.path.abspath(__file__))) 
URL_COLUMN_NAME = 'URL'

LOG_DIR_OUTPUT = os.path.join(BASE_PATH, 'logs')

OUTPUT_DIR_TEMP = os.path.join(BASE_PATH, 'html_pages_temp')
ZIP_OUTPUT_DIR = os.path.join(BASE_PATH, 'coletas_compactadas')

# --- LOG_FILE e ERROR_LOG_FILE apontam para logs/ ---
LOG_FILE = os.path.join(LOG_DIR_OUTPUT, 'collection_log.csv')
ERROR_LOG_FILE = os.path.join(LOG_DIR_OUTPUT, 'error_log.txt') 

# Configurando caminho para os datasets
DATASETS_DIR = 'E:/Documentos/RIWRS_2/datasets'
URL_FILES = [
    os.path.join(DATASETS_DIR, 'taruntiwarihp_dataset.csv'),
    os.path.join(DATASETS_DIR, 'mendeley_dataset.csv')
]

# --- Configura칞칫es do Coletor ---
MAX_WORKERS = 15
TIMEOUT_SECONDS = 30

os.makedirs(OUTPUT_DIR_TEMP, exist_ok=True)
os.makedirs(ZIP_OUTPUT_DIR, exist_ok=True)
os.makedirs(LOG_DIR_OUTPUT, exist_ok=True)

logger.info(f"Pasta de trabalho tempor치ria pronta em: '{OUTPUT_DIR_TEMP}'")
logger.info(f"Arquivos .zip ser칚o salvos em: '{ZIP_OUTPUT_DIR}'")
logger.info(f"Logs de resultados (CSV) ser칚o salvos em: '{LOG_DIR_OUTPUT}'")


# ==============================================================================
# 3. FUN칂츾O DE FINALIZA칂츾O (Compactar e Salvar Localmente)
# ==============================================================================
def finalize_collection():
    # ... (O corpo da fun칞칚o permanece o mesmo) ...
    logger.info("\n" + "="*50)
    logger.info("INICIANDO PROCESSO DE FINALIZA칂츾O...")
    logger.info("="*50)

    if not os.path.isdir(OUTPUT_DIR_TEMP) or not os.listdir(OUTPUT_DIR_TEMP):
        warnings.warn("Nenhum arquivo novo foi baixado nesta sess칚o. Nenhum arquivo .zip ser치 criado.", UserWarning)
        return

    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    zip_filename_base = f'coleta_html_{timestamp}'
    zip_path_final = os.path.join(ZIP_OUTPUT_DIR, zip_filename_base)
    
    logger.info(f"Compactando arquivos para: {zip_path_final}.zip")
    
    try:
        shutil.make_archive(zip_path_final, 'zip', OUTPUT_DIR_TEMP)
        logger.info("\nSucesso!")
        logger.info(f"Arquivo compactado salvo em: {zip_path_final}.zip")
        logger.info(f"Limpando a pasta tempor치ria '{OUTPUT_DIR_TEMP}'...")
        shutil.rmtree(OUTPUT_DIR_TEMP)
    except Exception as e:
        logger.error(f"Ocorreu um erro ao compactar os arquivos: {e}")

# ==============================================================================
# 4. FUN칂츾O PRINCIPAL (ORQUESTRADOR DA COLETA)
# ==============================================================================
def main():
    # ... (O corpo da fun칞칚o main permanece o mesmo, incluindo a l칩gica de coleta) ...
    logger.info("="*50)
    logger.info("INICIANDO PROCESSO DE COLETA DE P츼GINAS HTML")
    logger.info("="*50)
    
    Verificador.set_config(OUTPUT_DIR_TEMP, TIMEOUT_SECONDS)

    # ... (leitura de log, concatena칞칚o de CSVs, desduplica칞칚o) ...
    
    # ... (leitura de log e de URLs) ...
    completed_urls = set()
    try:
        if os.path.exists(LOG_FILE):
            log_df = pd.read_csv(LOG_FILE, on_bad_lines='skip')
            success_df = log_df[log_df['status'].str.startswith('SUCCESS', na=False)]
            completed_urls = set(success_df['original_url'])
    except Exception as e:
        warnings.warn(f"Falha ao ler o log de URLs conclu칤das: {e}. A coleta pode processar URLs duplicadas.", UserWarning)

    # --- Leitura e Concatena칞칚o dos M칰ltiplos Arquivos CSV ---
    all_dfs = []
    
    try:
        df_generator = (pd.read_csv(f, engine='python', on_bad_lines='skip') for f in URL_FILES)
        all_dfs = [df for df in df_generator]
        df = pd.concat(all_dfs, ignore_index=True)
        initial_count = len(df)
        df.drop_duplicates(subset=[URL_COLUMN_NAME], keep='first', inplace=True) 

        duplicates_removed = initial_count - len(df)
        if duplicates_removed > 0:
            logger.info(f"Desduplica칞칚o de URLs conclu칤da: {duplicates_removed} URLs duplicadas foram removidas.")
   
    except Exception as e:
        logger.error(f"Erro CR칈TICO ao ler e concatenar os arquivos de URLs: {e}")
        return False, []

    # Processamento e Filtragem de URLs
    raw_urls = df[URL_COLUMN_NAME].dropna().unique().tolist()
    urls = []
    for u in raw_urls:
        if isinstance(u, str):
            if not u.startswith(('http://', 'https://')):
                urls.append(f'http://{u}')
            else:
                urls.append(u)
    
    total_urls = len(urls)
    urls_to_collect = [url for url in urls if url not in completed_urls]
    print_summary(total_urls, len(completed_urls), len(urls_to_collect))

    if not urls_to_collect:
        logger.info("Nenhuma URL nova para coletar. A tarefa j치 foi conclu칤da!")
        return True, []
    
    needs_header = not os.path.exists(LOG_FILE)
    
    with open(LOG_FILE, 'a', encoding='utf-8', newline='') as log_f:
        if needs_header:
            log_f.write("original_url,saved_filename,status\n")

        with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
            future_to_url = {executor.submit(Verificador.download_url, url): url for url in urls_to_collect}
            for future in tqdm(as_completed(future_to_url), total=len(urls_to_collect), desc="Coletando P치ginas Restantes"):
                try:
                    original_url, saved_filename, status = future.result()
                    log_f.write(f'"{original_url}","{saved_filename}","{status}"\n')
                except Exception as exc:
                    url_falha = future_to_url[future]
                    logger.critical(f"FATAL_ERROR no Executor para {url_falha}: {exc}")
                    log_f.write(f'"{url_falha}","","FATAL_ERROR_during_future_result_{exc}"\n')

    logger.info(f"\nCOLETA DE P츼GINAS FINALIZADA!")
    return True, urls_to_collect

def print_summary(total, completed, remaining):
    logger.info("-" * 50)
    logger.info(f"Resumo da Tarefa:")
    logger.info(f"Total de URLs na fonte: {total}")
    logger.info(f"URLs j치 completas (de execu칞칫es anteriores): {completed}")
    logger.info(f"URLs a serem processadas nesta sess칚o: {remaining}")
    logger.info("-" * 50)


# ==============================================================================
# 5. L칍GICA DE P칍S-PROCESSAMENTO (Chamada pelo Launcher)
# ==============================================================================
def run_post_processing(collection_successful, attempted_urls):
    """Executa a gera칞칚o de relat칩rios e a finaliza칞칚o ap칩s a coleta."""
    
    if collection_successful:
        
        logger.info("\n" + "="*50)
        logger.info("GERANDO RELAT칍RIOS CONSOLIDADOS (SUCESSO/ERRO)")
        logger.info("="*50)
        
        # Chama a classe GeradorRelatorio para analisar o log completo
        relatorio_status = GeradorRelatorio.gerar_relatorios_consolidados(LOG_FILE, BASE_PATH)
        
        if relatorio_status["success_path"]:
           logger.info(f"Relat칩rio de Sucesso (Total: {relatorio_status['success_count']}) salvo em: {relatorio_status['success_path']}")
        if relatorio_status["error_path"]:
            logger.info(f"Relat칩rio de Erros (Total: {relatorio_status['error_count']}) salvo em: {relatorio_status['error_path']}")
        
        # Gera칞칚o da lista de URLs que falharam nesta sess칚o (error_log.txt)
        if attempted_urls: 
            try:
                log_df = pd.read_csv(LOG_FILE, on_bad_lines='skip')
                session_df = log_df[log_df['original_url'].isin(attempted_urls)]
                error_urls = session_df[session_df['status'].str.startswith(('ERROR', 'FATAL_ERROR'), na=False)]['original_url']
                
                if not error_urls.empty:
                    with open(ERROR_LOG_FILE, 'w', encoding='utf-8') as f:
                        for url in error_urls:
                            f.write(f"{url}\n")
                    logger.info(f"游늯 Uma lista com as {len(error_urls)} URLs que falharam *nesta sess칚o* foi salva em: '{ERROR_LOG_FILE}'")
            except Exception as e:
                logger.error(f"Falha ao gerar o log de erros da sess칚o: {e}")

        # Chama a fun칞칚o de finaliza칞칚o (compactar e salvar/limpar).
        finalize_collection()